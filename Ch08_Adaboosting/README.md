### Adaboost算法

**主要特点**：从弱学习器出发，反复学习，得到一系列弱学习器，然后组合这些弱学习器得到强分类器。大多数提升方法都是改变训练数据的概率分布(训练数据的权值分布)。

- 每一轮学习到的弱分类器，赋予的样本权值都不同，具体是根据该分类器的误差率及错误分类的样本得到
- 最终的模型由一系列加权的弱分类器表示得到

**与前向分步算法的关系**：Adaboost是前向分步加法算法的特例，这时，模型是由基本分类器组成的加法模型，损失函数是指数函数。